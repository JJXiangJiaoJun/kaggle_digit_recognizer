{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 采用TensorFlow 来搭建CNN进行手写数字预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = red>首先对数据进行预处理<br>\n",
    "    1、将数据图像标准化<br>\n",
    "    2、将数据打包成为mini-batch<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class get_mini_batch(object):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self,data,batch_size,image_height,image_width,channel=1):\n",
    "        '''\n",
    "        data:DataFrame数据类型，为读入的数据，第一列为标签，之后为像素数\n",
    "        batch_size:每一个batch的图片数\n",
    "        image_height:图片的高度\n",
    "        image_width:图片的宽度\n",
    "        channel:图片的通道数量\n",
    "        '''\n",
    "        #提取出标签，和图片数据\n",
    "        #x_train ,y_train 均为矩阵\n",
    "        \n",
    "        y_train = data.label.values\n",
    "       #将像素归一化为【0:1】\n",
    "        x_train = data.drop('label',axis=1).values.astype(np.float32)\n",
    "        x_train = x_train/x_train.max()\n",
    "        \n",
    "        #将y转换为 42000个数据  28*28 1通道的图像\n",
    "        x_train = x_train.reshape(-1,image_height,image_width,channel)\n",
    "        \n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.mini_batch_size = batch_size\n",
    "        self.index_in_epoch = 0\n",
    "        self.current_epoch =0.0\n",
    "        self.select_array = np.array([])\n",
    "    def next_batch(self):\n",
    "        '''\n",
    "        return: x_train_data_batch , y_train_data_batch\n",
    "        '''\n",
    "        start = self.index_in_epoch\n",
    "        self.index_in_epoch += self.mini_batch_size\n",
    "        self.current_epoch += self.mini_batch_size/(len(self.x_train))\n",
    "        \n",
    "        #将选择数组扩充到与训练样本一样的长度\n",
    "        if not len(self.select_array) == len(self.x_train):\n",
    "            self.select_array = np.arange(len(self.x_train))\n",
    "        \n",
    "        #r若是第一次取batch，则打乱顺序取\n",
    "        if  start == 0:\n",
    "            np.random.shuffle(self.select_array)\n",
    "        \n",
    "        #若到了数据尾部,则打乱重新开始选择\n",
    "        if self.index_in_epoch>self.x_train.shape[0]:\n",
    "            start = 0\n",
    "            np.random.shuffle(self.select_array)\n",
    "            self.index_in_epoch = self.mini_batch_size\n",
    "        end = self.index_in_epoch\n",
    "        \n",
    "        #至此已经选出mini-batch所以要对image进行标准化\n",
    "        x_tr = self.x_train[self.select_array[start:end]]\n",
    "        y_tr = self.y_train[self.select_array[start:end]]\n",
    "        \n",
    "        return x_tr,y_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  建立TensorFlow CNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(images , batch_size ,n_classes):\n",
    "    '''\n",
    "    参数说明:\n",
    "    images：输入图像batch 4D-tensor shape =[batchsize,image_width,image_height,channel]\n",
    "    batch_size : 每一批图像数量\n",
    "    n_classes: 输出类别数\n",
    "    \n",
    "    返回值:\n",
    "     output tensor with the computed logits, float, [batch_size, n_classes]\n",
    "    '''\n",
    "    # CONV1  16个 3*3 大小的卷积核\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        #创建16 3*3 大小的卷积核,shape = [filter_height, filter_width, \n",
    "        #                                   in_channels, out_channels]\n",
    "        weights = tf.get_variable('weights',\n",
    "                                   shape= [3,3,1,16],\n",
    "                                   dtype=tf.float32,\n",
    "                                   initializer=tf.truncated_normal_initializer(stddev=0.1,dtype=tf.float32))\n",
    "        biases  = tf.get_variable('biases',\n",
    "                                   shape= [16],\n",
    "                                   dtype=tf.float32,\n",
    "                                   initializer=tf.constant_initializer(0.1))\n",
    "        conv   = tf.nn.conv2d(images,weights,[1,1,1,1],padding='SAME')\n",
    "        pre_activation = tf.nn.bias_add(conv,biases)\n",
    "        conv1  = tf.nn.relu(pre_activation ,name =scope.name)\n",
    "    # pool1 and norm1 池化层1和标准化层1\n",
    "    with tf.variable_scope('pooling1_lrn') as scope:\n",
    "        \n",
    "        pool1 = tf.nn.max_pool(conv1,ksize =[1,2,2,1],strides=[1,2,2,1],\n",
    "                              padding='SAME',name='pooling1')\n",
    "        norm1 = tf.nn.lrn(pool1,depth_radius=4,bias=1.0,alpha=0.001/9.0,\n",
    "                         beta=0.75,name='norm1')\n",
    "    #CONV2  同conv1\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        \n",
    "        weights = tf.get_variable('weights',\n",
    "                                  shape=[3,3,16,16],\n",
    "                                  dtype=tf.float32,\n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.1,dtype=tf.float32))\n",
    "        biases  = tf.get_variable('biases',\n",
    "                                   shape=[16],\n",
    "                                   dtype=tf.float32,\n",
    "                                   initializer=tf.constant_initializer(0.1))\n",
    "        \n",
    "        conv = tf.nn.conv2d(norm1,weights,[1,1,1,1],padding='SAME')\n",
    "        pre_activation = tf.nn.bias_add(conv,biases)\n",
    "        conv2 = tf.nn.relu(pre_activation,name = scope.name)\n",
    "    \n",
    "    #pooling2 and norm2\n",
    "    with tf.variable_scope('pooling2_lrn') as scope:\n",
    "        norm2 = tf.nn.lrn(conv2,depth_radius=4, bias=1.0, alpha=0.001/9.0,\n",
    "                          beta=0.75,name='norm2')\n",
    "        pool2 = tf.nn.max_pool(conv2,ksize=[1,2,2,1],strides=[1,1,1,1],\n",
    "                               padding='SAME',name='pool2')\n",
    "        \n",
    "    #full-connect1\n",
    "    with tf.variable_scope('fc1')  as scope:\n",
    "        #将池化层输出转化为每一行为一个样本\n",
    "        reshape = tf.reshape(pool2,shape = [batch_size,-1])\n",
    "        dim = reshape.get_shape()[1].value\n",
    "        \n",
    "        weights = tf.get_variable('weights',\n",
    "                                   shape = [dim,64],\n",
    "                                   dtype = tf.float32,\n",
    "                                   initializer=tf.truncated_normal_initializer(stddev=0.005,dtype=tf.float32))\n",
    "        biases  = tf.get_variable('biases',\n",
    "                                   shape = [64],\n",
    "                                   dtype = tf.float32,\n",
    "                                   initializer= tf.constant_initializer(0.1))\n",
    "        fc1 = tf.nn.relu(tf.matmul(reshape,weights)+biases,name=scope.name)\n",
    "     \n",
    "    #full-connect2\n",
    "    with tf.variable_scope('fc2') as scope:\n",
    "        \n",
    "        weights = tf.get_variable('weights',\n",
    "                                   shape=[64,64],\n",
    "                                   dtype=tf.float32,\n",
    "                                   initializer=tf.truncated_normal_initializer(stddev=0.005,dtype=tf.float32))\n",
    "        biases  = tf.get_variable('biases',\n",
    "                                  shape=[64],\n",
    "                                  dtype=tf.float32,\n",
    "                                  initializer=tf.constant_initializer(0.1))\n",
    "        fc2 = tf.nn.relu(tf.matmul(fc1,weights)+biases,name=scope.name)\n",
    "        \n",
    "        \n",
    "    #soft-max output\n",
    "    with tf.variable_scope('soft_max') as scope:\n",
    "        \n",
    "        weights =tf.get_variable('weights',\n",
    "                                 shape = [64,n_classes],\n",
    "                                 dtype =tf.float32,\n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.005,dtype=tf.float32))\n",
    "        biases = tf.get_variable('biases',\n",
    "                                 shape=[n_classes],\n",
    "                                dtype=tf.float32,\n",
    "                                initializer=tf.constant_initializer(0.1))\n",
    "        softmax_linear = tf.add(tf.matmul(fc2,weights),biases,name=scope.name)\n",
    "        \n",
    "    \n",
    "    return softmax_linear\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def losses(logits,labels):\n",
    "    '''\n",
    "    参数说明:\n",
    "    logits:logits tensor ,float ,[batch_size,n_classes]\n",
    "    labels:label  tensor ,tf.int32,[batch_size]\n",
    "    \n",
    "    返回值:\n",
    "    loss tensor float type\n",
    "     \n",
    "    '''\n",
    "    \n",
    "    with tf.variable_scope('loss') as scope:\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits\\\n",
    "                        (logits=logits, labels=labels, name='xentropy_per_example')\n",
    "        loss = tf.reduce_mean(cross_entropy,name='loss')\n",
    "        tf.summary.scalar(scope.name+'/loss',loss)\n",
    "    return loss\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainning(loss,learning_rate):\n",
    "    \n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        global_step = tf.Variable(0,name='global_step',trainable=False)\n",
    "        train_op =optimizer.minimize(loss,global_step=global_step)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(logits,labels):\n",
    "    with tf.variable_scope('accuracy') as scope:\n",
    "        correct = tf.nn.in_top_k(logits,labels,1)\n",
    "        correct = tf.cast(correct,tf.float16)\n",
    "        accuracy = tf.reduce_mean(correct)\n",
    "        tf.summary.scalar(scope.name+'/accuracy',accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  对模型进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#每个minibatch的图片数量\n",
    "BATCH_SIZE = 50\n",
    "#图片的长和宽\n",
    "IMG_H = 28\n",
    "IMG_W = 28\n",
    "n_classes = 10\n",
    "MAX_STEP  = 10000\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    train_data = pd.read_csv('data/train.csv')\n",
    "    logs_train_dir = 'logs/train/'\n",
    "    \n",
    "    \n",
    "    x_train = tf.placeholder(tf.float32,shape = [BATCH_SIZE,IMG_W,IMG_H,1])\n",
    "    y_train = tf.placeholder(tf.int64,shape=[BATCH_SIZE])\n",
    "    \n",
    "    \n",
    "    batch_generater = get_mini_batch(train_data,BATCH_SIZE,IMG_H,IMG_W,channel=1)\n",
    "    logits  = inference(x_train,BATCH_SIZE,n_classes)\n",
    "    loss    = losses(logits,y_train)\n",
    "    acc     = evaluation(logits,y_train)\n",
    "    train_op = trainning(loss,learning_rate=learning_rate)\n",
    "    \n",
    "    \n",
    "    \n",
    "    with tf.Session()  as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        summary_op =tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter(logs_train_dir,sess.graph)\n",
    "        print('Start Trainning')\n",
    "        try:\n",
    "            for step in np.arange(MAX_STEP):\n",
    "                tra_image ,tra_label = batch_generater.next_batch()\n",
    "                _ , tra_loss ,tra_acc =sess.run([train_op,loss,acc],\n",
    "                                                feed_dict={x_train:tra_image,y_train:tra_label})\n",
    "                if (batch_generater.current_epoch - int(batch_generater.current_epoch))==0:\n",
    "                    print('Epoch %d , train loss = %.2f , train accuracy=%.2f%%'%(int(batch_generater.current_epoch),tra_loss,tra_acc*100))\n",
    "                \n",
    "                if step % 2000 == 0 or (step+1)==MAX_STEP:\n",
    "                    checkpoint_path = os.path.join(logs_train_dir,'model.ckpt')\n",
    "                    saver.save(sess,checkpoint_path,global_step=step)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('Done trainning -- epoch limit reached')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable conv1/weights already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n  File \"C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-2f18cc448813>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-1dd863926cf5>\u001b[0m in \u001b[0;36mrun_training\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mbatch_generater\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_mini_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mIMG_H\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mIMG_W\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mchannel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mlogits\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mloss\u001b[0m    \u001b[1;33m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0macc\u001b[0m     \u001b[1;33m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-60c7bb8307d1>\u001b[0m in \u001b[0;36minference\u001b[1;34m(images, batch_size, n_classes)\u001b[0m\n\u001b[0;32m     16\u001b[0m                                    \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                                    \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m                                    initializer=tf.truncated_normal_initializer(stddev=0.1,dtype=tf.float32))\n\u001b[0m\u001b[0;32m     19\u001b[0m         biases  = tf.get_variable('biases',\n\u001b[0;32m     20\u001b[0m                                    \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m   1315\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1317\u001b[1;33m       constraint=constraint)\n\u001b[0m\u001b[0;32m   1318\u001b[0m get_variable_or_local_docstring = (\n\u001b[0;32m   1319\u001b[0m     \"\"\"%s\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m   1077\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1078\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1079\u001b[1;33m           constraint=constraint)\n\u001b[0m\u001b[0;32m   1080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m    423\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 425\u001b[1;33m           constraint=constraint)\n\u001b[0m\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[0;32m    392\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[0;32m    731\u001b[0m                          \u001b[1;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    732\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[1;32m--> 733\u001b[1;33m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[0;32m    734\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable conv1/weights already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n  File \"C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n"
     ]
    }
   ],
   "source": [
    "run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-9063a9f0e032>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
